<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>理解Transformer模型2：训练Transformer - 子实</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="子实"><meta name="msapplication-TileImage" content="/img/avatar-o.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="子实"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="尝试通过手写transformer来理解该算法"><meta property="og:type" content="article"><meta property="og:title" content="理解Transformer模型2：训练Transformer"><meta property="og:url" content="https://zlotus.github.io/2024/01/13/transformer-from-scratch-2/"><meta property="og:site_name" content="子实"><meta property="og:description" content="尝试通过手写transformer来理解该算法"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://zlotus.github.io/2024/01/13/transformer-from-scratch-2/transformer-from-scratch-2-00-the-transformer.png"><meta property="og:image" content="https://zlotus.github.io/2024/01/13/transformer-from-scratch-2/transformer-from-scratch-2-01-what-is-an-input-embedding.png"><meta property="og:image" content="https://zlotus.github.io/2024/01/13/transformer-from-scratch-2/transformer-from-scratch-2-02-self-attention-in-detail.png"><meta property="article:published_time" content="2024-01-13T10:20:00.000Z"><meta property="article:modified_time" content="2024-01-13T10:20:00.000Z"><meta property="article:author" content="子实"><meta property="article:tag" content="Python"><meta property="article:tag" content="Linux"><meta property="article:tag" content="llama"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/2024/01/13/transformer-from-scratch-2/transformer-from-scratch-2-00-the-transformer.png"><meta property="twitter:creator" content="@Zealot_uS"><meta property="twitter:site" content="Zealot_uS"><link rel="publisher" href="+LotusQin"><meta property="fb:admins" content="qinzishi"><meta property="fb:app_id" content="qinzishi"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zlotus.github.io/2024/01/13/transformer-from-scratch-2/"},"headline":"理解Transformer模型2：训练Transformer","image":["https://zlotus.github.io/2024/01/13/transformer-from-scratch-2/transformer-from-scratch-2-00-the-transformer.png","https://zlotus.github.io/2024/01/13/transformer-from-scratch-2/transformer-from-scratch-2-01-what-is-an-input-embedding.png","https://zlotus.github.io/2024/01/13/transformer-from-scratch-2/transformer-from-scratch-2-02-self-attention-in-detail.png"],"datePublished":"2024-01-13T10:20:00.000Z","dateModified":"2024-01-13T10:20:00.000Z","author":{"@type":"Person","name":"子实"},"publisher":{"@type":"Organization","name":"子实","logo":{"@type":"ImageObject","url":"https://zlotus.github.io/img/opm_sq.png"}},"description":"尝试通过手写transformer来理解该算法"}</script><link rel="icon" href="/img/avatar-o.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css"><!--!--><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/opm_sq.png" alt="子实" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/2014/07/09/about-me/">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/zlotus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-9-tablet is-9-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-01-13T10:20:00.000Z" title="1/13/2024, 6:20:00 PM">2024-01-13</time>发表</span><span class="level-item"><time dateTime="2024-01-13T10:20:00.000Z" title="1/13/2024, 6:20:00 PM">2024-01-13</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">40 分钟读完 (大约6029个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">理解Transformer模型2：训练Transformer</h1><div class="content"><p>接着上一篇文章，任务目标依然是通过使用Transformer将英语翻译为意大利语，来理解Transformer是如何编写和训练的，同时本文还将通过可视化观察注意力模型的细节。文中将使用<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/opus_books">Hugging Face的opus_books</a>作为训练集，通过Hugging Face的工具链完成数据集的下载，和将文本转换为词表的工作。</p>
<img src="/2024/01/13/transformer-from-scratch-2/transformer-from-scratch-2-00-the-transformer.png" class="" width="270" height="480">

<span id="more"></span>

<h2 id="1-Tokenizer"><a href="#1-Tokenizer" class="headerlink" title="1 Tokenizer"></a>1 Tokenizer</h2><p>观察数据集，可以发现Hugging Face提供的数据都是成对出现的原文（英语）-译文（意大利语）字典。第一步是下载数据集，并创建tokenizer（也译为分词）。</p>
<img src="/2024/01/13/transformer-from-scratch-2/transformer-from-scratch-2-01-what-is-an-input-embedding.png" class="" width="480" height="270">

<br>

<p>Tokenizer在Input Embedding的输入之前，用于将句子拆分成token从而构建词表（其中还将包括用于让模型识别的特殊token，如用于padding的、用于标识句子起止位置的等等），<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/tokenizer_summary">tokenizer的种类有很多</a>，如<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=HEikzVL-lZU">BPE tokenizer</a>（按频度统计分出词根）、<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=nhJxYji1aho">Word tokenizer</a>（按空格和标点分词）、<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=zHvTiHr506c">Subword tokenizer</a>（高频词不分，低频词分出有意义的subword或词根）。本文以教学为目标，因此选择使用最简单的Word tokenizer。</p>
<p>新建文件<code>train.py</code>用于训练模型。</p>
<p>安装Hugging Face的<code>datasets</code>和<code>tokenizer</code>库用于下载数据集和调用分词器（<code>pip</code>和<code>conda</code>都可以）。</p>
<p>编写函数用于创建分词器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader, random_split</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> Tokenizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tokenizers.models <span class="keyword">import</span> WordLevel</span><br><span class="line"><span class="keyword">from</span> tokenizers.trainers <span class="keyword">import</span> WordLevelTrainer <span class="comment"># tokenizer相对应的trainer，通过给定的句子创建词表</span></span><br><span class="line"><span class="keyword">from</span> tokenizers.pre_tokenizers <span class="keyword">import</span> Whitespace <span class="comment"># 按空格分词</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_or_build_tokenizer</span>(<span class="params">config, ds, lang</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    创建分词器</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数: </span></span><br><span class="line"><span class="string">        `config`: 模型的配置</span></span><br><span class="line"><span class="string">        `ds`: 数据集</span></span><br><span class="line"><span class="string">        `lang`: 分词器的语言</span></span><br><span class="line"><span class="string">    返回: </span></span><br><span class="line"><span class="string">        `Tokenizer`分词器实例</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># config[&#x27;tokenizer_file&#x27;] = &#x27;../tokenizer/tokenizer_&#123;0&#125;.json&#x27;</span></span><br><span class="line">    tokenizer_path = Path(config[<span class="string">&#x27;tokenizer_file&#x27;</span>].<span class="built_in">format</span>(lang))</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> Path.exists(tokenizer_path):</span><br><span class="line">        tokenizer = Tokenizer(WordLevel(unk_token=<span class="string">&#x27;[UNK]&#x27;</span>)) <span class="comment"># 分词器在遇到不在词表中的词时将其替换为`[UNK]`</span></span><br><span class="line">        tokenizer.pre_tokenizer = Whitespace()</span><br><span class="line">        trainer = WordLevelTrainer(special_tokens=[<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[SOS]&quot;</span>, <span class="string">&quot;[EOS]&quot;</span>], min_frequency=<span class="number">2</span>) <span class="comment"># 一个词要出现在词表中，需要至少在句子中出现两次</span></span><br><span class="line">        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokenizer = Tokenizer.from_file(<span class="built_in">str</span>(tokenizer_path))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tokenizer</span><br></pre></td></tr></table></figure>

<p>以及从数据集中生成句子的工具函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_sentences</span>(<span class="params">ds, lang</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    从数据集中取指定语言的句子</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数: </span></span><br><span class="line"><span class="string">        `ds`: 数据集</span></span><br><span class="line"><span class="string">        `lang`: 指定语言 `&quot;en&quot;`或`&quot;it&quot;`</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> ds:</span><br><span class="line">        <span class="keyword">yield</span> item[<span class="string">&#x27;translation&#x27;</span>][lang]</span><br></pre></td></tr></table></figure>

<p>在Hugging Face中下载数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ds</span>(<span class="params">config</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    加载数据集并创建分词器</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在Hugging Face中下载opus_books数据集中的`en-it`子集，选择`train`部分，我们之后将自行再分出validation部分</span></span><br><span class="line">    ds_raw = load_dataset(<span class="string">&#x27;opus_books&#x27;</span>, <span class="string">f&#x27;<span class="subst">&#123;config[<span class="string">&quot;lang_src&quot;</span>]&#125;</span>-<span class="subst">&#123;config[<span class="string">&quot;lang_tgt&quot;</span>]&#125;</span>&#x27;</span>, split=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建分词器</span></span><br><span class="line">    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[<span class="string">&#x27;lang_src&#x27;</span>])</span><br><span class="line">    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[<span class="string">&#x27;lang_tgt&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将数据集分为90%的训练集和10%的验证集</span></span><br><span class="line">    train_ds_size = <span class="built_in">int</span>(<span class="number">0.9</span> * <span class="built_in">len</span>(ds_raw))</span><br><span class="line">    val_ds_size = <span class="built_in">len</span>(ds_raw) - train_ds_size</span><br><span class="line">    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])</span><br></pre></td></tr></table></figure>

<p>此时需要处理数据集训练模型，我们先编写<code>dataset.py</code>为模型准备数据。</p>
<h2 id="2-Dataset"><a href="#2-Dataset" class="headerlink" title="2 Dataset"></a>2 Dataset</h2><p>新建文件<code>dataset.py</code>，以生成用于输入模型的由张量组成的数据集。初始化<code>BilingualDataset</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BilingualDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.ds = ds</span><br><span class="line">        self.tokenizer_src = tokenizer_src</span><br><span class="line">        self.tokenizer_tgt = tokenizer_tgt</span><br><span class="line">        self.src_lang = src_lang</span><br><span class="line">        self.tgt_lang = tgt_lang</span><br><span class="line">        self.seq_len = seq_len</span><br><span class="line"></span><br><span class="line">        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(<span class="string">&quot;[SOS]&quot;</span>)], dtype=torch.int64)</span><br><span class="line">        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(<span class="string">&quot;[EOS]&quot;</span>)], dtype=torch.int64)</span><br><span class="line">        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(<span class="string">&quot;[PAD]&quot;</span>)], dtype=torch.int64)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span> <span class="comment"># 数据集大小</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.ds)</span><br></pre></td></tr></table></figure>

<p>然后编写关键的函数<code>__getitem__</code>，用于将原始数据转换为张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">    src_target_pair = self.ds[index]</span><br><span class="line">    src_text = src_target_pair[<span class="string">&#x27;translation&#x27;</span>][self.src_lang]</span><br><span class="line">    tgt_text = src_target_pair[<span class="string">&#x27;translation&#x27;</span>][self.tgt_lang]</span><br></pre></td></tr></table></figure>

<p>先将文本转换为token，再转换成id，即tokenizer先将句子拆分成词，再将词转换成词表中的id：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将文本转换成token，再转换成id，即tokenizer先将句子拆分成词，再将词转换成词表中的id</span></span><br><span class="line">enc_input_tokens = self.tokenizer_src.encode(src_text).ids</span><br><span class="line">dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids</span><br></pre></td></tr></table></figure>

<p>然后计算需要填充（padding）的token数量，以使句子长度总能达到<code>seq_len</code>。此外，因为decoder的输入只有<code>[SOS]</code>没有<code>[EOS]</code>，而decoder的输出（也叫做label，即期望的翻译结果）只有<code>[EOS]</code>没有<code>[SOS]</code>，所以此处的padding会多一个token：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 填充句子，使其长度达到seq_len</span></span><br><span class="line">enc_num_padding_tokens = self.seq_len - <span class="built_in">len</span>(enc_input_tokens) - <span class="number">2</span></span><br><span class="line"><span class="comment"># 因为decoder的输入只有sos没有eos，所以padding要多一个token</span></span><br><span class="line">dec_num_padding_tokens = self.seq_len - <span class="built_in">len</span>(dec_input_tokens) - <span class="number">1</span> </span><br></pre></td></tr></table></figure>

<p>这里需要确保确保选择的<code>seq_len</code>长度满足所有样本，即填充的token数量应该不为负数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确保选择的seq_len长度满足所有样本，即padding应该不为负数</span></span><br><span class="line"><span class="keyword">if</span> enc_num_padding_tokens &lt; <span class="number">0</span> <span class="keyword">or</span> dec_num_padding_tokens &lt; <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Sentence is too long&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>为encoder的输入组装tensor，依次为：<code>&#39;[SOS]&#39;</code>、输入tensor、<code>&#39;[EOS]&#39;</code>和填充<code>&#39;[PAD]&#39;</code>*<code>enc_num_padding_tokens</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为encoder的输入填充tensor，添加&#x27;[SOS]&#x27;、&#x27;[EOS]&#x27;、&#x27;[PAD]&#x27;</span></span><br><span class="line">encoder_input = torch.cat([</span><br><span class="line">    self.sos_token,</span><br><span class="line">    torch.tensor(enc_input_tokens, dtype=torch.int64),</span><br><span class="line">    self.eos_token,</span><br><span class="line">    torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p>为decoder的输入组装tensor，依次为：<code>&#39;[SOS]&#39;</code>、输入tensor和填充<code>&#39;[PAD]&#39;</code>*<code>enc_num_padding_tokens</code>（没有<code>&#39;[EOS]&#39;</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为decoder的输入填充tensor，只添加&#x27;[SOS]&#x27;和&#x27;[PAD]&#x27;</span></span><br><span class="line">decoder_input = torch.cat([</span><br><span class="line">    self.sos_token,</span><br><span class="line">    torch.tensor(dec_input_tokens, dtype=torch.int64),</span><br><span class="line">    torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p>为decoder的输出（即label）组装tensor，依次为：输入tensor、<code>&#39;[EOS]&#39;</code>和填充<code>&#39;[PAD]&#39;</code>*<code>enc_num_padding_tokens</code>（没有<code>&#39;[SOS]&#39;</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为decoder的输出（也叫做label，即期望的翻译结果）填充tensor，只添加&#x27;[EOS]&#x27;</span></span><br><span class="line">label = torch.cat([</span><br><span class="line">    torch.tensor(dec_input_tokens, dtype=torch.int64),</span><br><span class="line">    self.eos_token,</span><br><span class="line">    torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p>此处再次检查填充后的tensor长度是否满足<code>seq_len</code>，并返回结果。其中，encoder的mask仅用于屏蔽掉填充的token，而decoder的mask用于屏蔽掉填充的token和未来的token：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 再次检测填充后的tensor长度是否满足seq_len</span></span><br><span class="line"><span class="keyword">assert</span> encoder_input.size(<span class="number">0</span>) == self.seq_len</span><br><span class="line"><span class="keyword">assert</span> decoder_input.size(<span class="number">0</span>) == self.seq_len</span><br><span class="line"><span class="keyword">assert</span> label.size(<span class="number">0</span>) == self.seq_len</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> &#123;</span><br><span class="line">    <span class="string">&#x27;encoder_input&#x27;</span>: encoder_input, <span class="comment"># (seq_len)</span></span><br><span class="line">    <span class="string">&#x27;decoder_input&#x27;</span>: decoder_input, <span class="comment"># (seq_len)</span></span><br><span class="line">    <span class="string">&#x27;encoder_mask&#x27;</span>: (encoder_input != self.pad_token).unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>).<span class="built_in">int</span>(), <span class="comment"># (1, 1, seq_len)</span></span><br><span class="line">    <span class="string">&#x27;decoder_mask&#x27;</span>: (decoder_input != self.pad_token).unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>).<span class="built_in">int</span>() &amp; causal_mask(decoder_input.size(<span class="number">0</span>)), <span class="comment"># (1, seq_len, seq_len)) </span></span><br><span class="line">    <span class="string">&#x27;label&#x27;</span>: label, <span class="comment"># (seq_len)</span></span><br><span class="line">    <span class="string">&#x27;src_text&#x27;</span>: src_text, <span class="comment"># 原文，用于可视化</span></span><br><span class="line">    <span class="string">&#x27;tgt_text&#x27;</span>: tgt_text, <span class="comment"># 译文，用于可视化</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>我们在上面使用了一个叫做<code>causal_mask</code>的函数，用于创建decoder的mask。该函数的作用是使得decoder只能看到之前的token，而不能看到未来的token。回顾一下Self-Attention的细节：</p>
<img src="/2024/01/13/transformer-from-scratch-2/transformer-from-scratch-2-02-self-attention-in-detail.png" class="" width="480" height="270">

<br>

<p>这个矩阵表示$Q\times K^T$，我们希望每个词只能看到它之前的词，因此需要使用mask隐藏矩阵对角元素之上的部分。在这个例子中，我们不希望<code>YOUR</code>看到<code>CAT</code>、<code>IS</code>、<code>A</code>、<code>LOVELY</code>、<code>CAT</code>，我们希望<code>YOUR</code>只能看到<code>YOUR</code>自己；而<code>LOVELY</code>则应该能看到它之前的词，即<code>YOUR</code>、<code>CAT</code>、<code>IS</code>、<code>LOVELY</code>，但看不到最后的词<code>CAT</code>。使用Pytorch可以方便的创建一个下三角矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">causal_mask</span>(<span class="params">size</span>):</span></span><br><span class="line">    <span class="comment"># triu函数中的diagonal=1表示主对角线+1，即生成上三角矩阵后主对角线再置为0</span></span><br><span class="line">    mask = torch.triu(torch.ones(<span class="number">1</span>, size, size), diagonal=<span class="number">1</span>).<span class="built_in">type</span>(torch.<span class="built_in">int</span>)</span><br><span class="line">    <span class="comment"># 再将矩阵取反，即生成下三角矩阵</span></span><br><span class="line">    <span class="keyword">return</span> mask == <span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>我们完成了<code>dataset.py</code>中准备数据集的工作，接下来可以继续完成<code>train.py</code>中的训练模型的循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataset <span class="keyword">import</span> BilingualDataset, causal_mask</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ds</span>(<span class="params">config</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    加载数据集并创建分词器</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在Hugging Face中下载opus_books数据集中的`en-it`子集，选择`train`部分，我们之后将自行再分出validation部分</span></span><br><span class="line">    ds_raw = load_dataset(<span class="string">&#x27;opus_books&#x27;</span>, <span class="string">f&#x27;<span class="subst">&#123;config[<span class="string">&quot;lang_src&quot;</span>]&#125;</span>-<span class="subst">&#123;config[<span class="string">&quot;lang_tgt&quot;</span>]&#125;</span>&#x27;</span>, split=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建分词器</span></span><br><span class="line">    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[<span class="string">&#x27;lang_src&#x27;</span>])</span><br><span class="line">    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[<span class="string">&#x27;lang_tgt&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将数据集分为90%的训练集和10%的验证集</span></span><br><span class="line">    train_ds_size = <span class="built_in">int</span>(<span class="number">0.9</span> * <span class="built_in">len</span>(ds_raw))</span><br><span class="line">    val_ds_size = <span class="built_in">len</span>(ds_raw) - train_ds_size</span><br><span class="line">    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])</span><br><span class="line"></span><br><span class="line">    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config[<span class="string">&#x27;lang_src&#x27;</span>], config[<span class="string">&#x27;lang_tgt&#x27;</span>], config[<span class="string">&#x27;seq_len&#x27;</span>])</span><br><span class="line">    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config[<span class="string">&#x27;lang_src&#x27;</span>], config[<span class="string">&#x27;lang_tgt&#x27;</span>], config[<span class="string">&#x27;seq_len&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 找出训练集中源语言和目标语言里最长的句子长度分别是多少</span></span><br><span class="line">    max_len_src = <span class="number">0</span></span><br><span class="line">    max_len_tgt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> ds_raw:</span><br><span class="line">        src_ids = tokenizer_src.encode(item[<span class="string">&#x27;translation&#x27;</span>][config[<span class="string">&#x27;lang_src&#x27;</span>]]).ids</span><br><span class="line">        tgt_ids = tokenizer_tgt.encode(item[<span class="string">&#x27;translation&#x27;</span>][config[<span class="string">&#x27;lang_tgt&#x27;</span>]]).ids</span><br><span class="line">        max_len_src = <span class="built_in">max</span>(max_len_src, <span class="built_in">len</span>(src_ids))</span><br><span class="line">        max_len_tgt = <span class="built_in">max</span>(max_len_tgt, <span class="built_in">len</span>(tgt_ids))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Max length of source language: <span class="subst">&#123;max_len_src&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Max length of target language: <span class="subst">&#123;max_len_tgt&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    train_dataloader = DataLoader(train_ds, batch_size=config[<span class="string">&#x27;batch_size&#x27;</span>], shuffle=<span class="literal">True</span>)</span><br><span class="line">    val_dataloader = DataLoader(val_ds, batch_size=<span class="number">1</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="3-Trainning-Loop"><a href="#3-Trainning-Loop" class="headerlink" title="3 Trainning Loop"></a>3 Trainning Loop</h2><p>到这里，我们已经准备好了模型和数据集，可以开始训练模型了。首先是创建模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> build_transformer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span>(<span class="params">config, vocab_src_len, vocab_tgt_len</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    创建模型</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model = build_transformer(vocab_src_len, vocab_tgt_len, config[<span class="string">&#x27;seq_len&#x27;</span>], config[<span class="string">&#x27;seq_len&#x27;</span>], config[<span class="string">&#x27;d_model&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<p>在前面的文章中，我们只是使用了很多次<code>config</code>但是从来没有定义这个模型配置文件，现在我们来定义这个配置文件，新建<code>config.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_config</span>():</span></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;batch_size&quot;</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">&quot;num_epochs&quot;</span>: <span class="number">20</span>,</span><br><span class="line">        <span class="comment"># 在实际的训练中，lr应该根据实际情况调整，比如一开始设置较大的lr，然后在训练过程中逐渐减小lr</span></span><br><span class="line">        <span class="comment"># 但本文的目的在于展示Transformer的原理，使用可变的lr会增加代码复杂度，因此这里使用固定的lr</span></span><br><span class="line">        <span class="string">&quot;lr&quot;</span>: <span class="number">10</span>**-<span class="number">4</span>,</span><br><span class="line">        <span class="string">&quot;seq_len&quot;</span>: <span class="number">350</span>,</span><br><span class="line">        <span class="string">&quot;d_model&quot;</span>: <span class="number">512</span>,</span><br><span class="line">        <span class="string">&quot;lang_src&quot;</span>: <span class="string">&quot;en&quot;</span>,</span><br><span class="line">        <span class="string">&quot;lang_tgt&quot;</span>: <span class="string">&quot;it&quot;</span>,</span><br><span class="line">        <span class="string">&quot;model_folder&quot;</span>: <span class="string">&quot;weights&quot;</span>,</span><br><span class="line">        <span class="string">&quot;model_basename&quot;</span>: <span class="string">&quot;tmodel_&quot;</span>,</span><br><span class="line">        <span class="comment"># 预加载模型，如果不为空，则从预加载模型中加载参数（如在训练过程中程序意外中断），否则从头开始训练</span></span><br><span class="line">        <span class="string">&quot;preload&quot;</span>: <span class="literal">None</span>,</span><br><span class="line">        <span class="comment"># 分词文件，如`_en.json`和`_it.json`</span></span><br><span class="line">        <span class="string">&quot;tokenizer_file&quot;</span>: <span class="string">&quot;tokenizer_&#123;0&#125;.json&quot;</span>,</span><br><span class="line">        <span class="comment"># 为tensorboard提供的log路径</span></span><br><span class="line">        <span class="string">&quot;experiment_name&quot;</span>: <span class="string">&quot;runs/tmodel&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weights_file_path</span>(<span class="params">config, epoch: <span class="built_in">str</span></span>):</span></span><br><span class="line">    model_folder = config[<span class="string">&quot;model_folder&quot;</span>]</span><br><span class="line">    model_basename = config[<span class="string">&quot;model_basename&quot;</span>]</span><br><span class="line">    model_filename = <span class="string">f&quot;<span class="subst">&#123;model_basename&#125;</span><span class="subst">&#123;epoch&#125;</span>.pt&quot;</span></span><br><span class="line">    <span class="comment"># Path重载了`/`操作符，可以直接使用`/`创建子目录</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">str</span>(Path(<span class="string">&#x27;.&#x27;</span>) / model_folder / model_filename)</span><br></pre></td></tr></table></figure>

<p>接下来开始编写训练模型的代码。本文使用tensorboard观察模型训练细节，要在本地使用tensorboard，可以安装<code>tensorboard</code>和<code>torch_tb_profiler</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorboard torch_tb_profiler</span><br></pre></td></tr></table></figure>

<p>然后在命令行中运行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir runs</span><br></pre></td></tr></table></figure>

<p>即可打开tensorboard，在浏览器中访问<code>http://localhost:6006/</code>即可查看tensorboard的界面。或是在vscode中安装<code>tensorboard</code>插件，然后点击代码中的“启动TensorBoard会话”按钮，即可打开tensorboard。</p>
<p>为预加载权重编写代码，若指定了预加载的模型，则直接加载。此外，指定loss函数，声明padding不参与loss计算。同时，使用label smoothing，让模型降低对计算结果的确定性，即减少本次推理结果的概率，并把减少的部分分配到其他可能的推理结果上。实测可以提升模型的泛化能力，降低过拟合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> get_weights_file_path, get_config</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span>(<span class="params">config</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练模型</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择设备</span></span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Using device: <span class="subst">&#123;device&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 确定权重文件目录存在</span></span><br><span class="line">    Path(config[<span class="string">&#x27;model_folder&#x27;</span>]).mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载数据集和分词器</span></span><br><span class="line">    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)</span><br><span class="line">    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 启动Tensorboard以可视化loss</span></span><br><span class="line">    writer = SummaryWriter(config[<span class="string">&#x27;experiment_name&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=config[<span class="string">&#x27;lr&#x27;</span>], eps=<span class="number">1e-9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 若指定了预加载权重，则加载预加载权重</span></span><br><span class="line">    initial_epoch = <span class="number">0</span></span><br><span class="line">    global_step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> config[<span class="string">&#x27;preload&#x27;</span>]:</span><br><span class="line">        model_filename = get_weights_file_path(config, config[<span class="string">&#x27;preload&#x27;</span>])</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Preloading model <span class="subst">&#123;model_filename&#125;</span>&#x27;</span>)</span><br><span class="line">        state = torch.load(model_filename)</span><br><span class="line">        initial_epoch = state[<span class="string">&#x27;epoch&#x27;</span>] + <span class="number">1</span></span><br><span class="line">        optimizer.load_state_dict(state[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line">        global_step = state[<span class="string">&#x27;global_step&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义loss函数，声明padding不参与loss计算。同时，使用label smoothing，让模型降低对计算结果的确定性，</span></span><br><span class="line">    <span class="comment"># 即减少本次推理结果的概率，并把减少的部分分配到其他可能的推理结果上。实测可以提升模型的泛化能力，降低过拟合。</span></span><br><span class="line">    <span class="comment"># 这里的label smoothing的系数设置为0.1，即将最高概率标签的概率降低0.1，再并分配给其他标签的概率。</span></span><br><span class="line">    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id(<span class="string">&#x27;[PAD]&#x27;</span>), label_smoothing=<span class="number">0.1</span>).to(device)</span><br></pre></td></tr></table></figure>

<p>编写trainning loop，并在每个epoch结束时保存模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(initial_epoch, config[<span class="string">&#x27;num_epochs&#x27;</span>]):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="comment"># 画进度条</span></span><br><span class="line">    batch_iterator = tqdm(train_dataloader, desc=<span class="string">f&quot;Processing epoch <span class="subst">&#123;epoch:02d&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> batch_iterator:</span><br><span class="line"></span><br><span class="line">        encoder_input = batch[<span class="string">&#x27;encoder_input&#x27;</span>].to(device) <span class="comment"># (batch, seq_len)</span></span><br><span class="line">        decoder_input = batch[<span class="string">&#x27;decoder_input&#x27;</span>].to(device) <span class="comment"># (batch, seq_len)</span></span><br><span class="line">        encoder_mask = batch[<span class="string">&#x27;encoder_mask&#x27;</span>].to(device) <span class="comment"># (batch, 1, 1, seq_len) 只隐藏padding tokens</span></span><br><span class="line">        decoder_mask = batch[<span class="string">&#x27;decoder_mask&#x27;</span>].to(device) <span class="comment"># (batch, 1, seq_len, seq_len) 隐藏padding tokens和未来的tokens</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将tensor输入Transformer模型并计算</span></span><br><span class="line">        encoder_output = model.encode(encoder_input, encoder_mask) <span class="comment"># (batch, seq_len, d_model)</span></span><br><span class="line">        decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) <span class="comment"># (batch, seq_len, d_model)</span></span><br><span class="line">        proj_output = model.project(decoder_output) <span class="comment"># (batch, seq_len, tgt_vocab_size)</span></span><br><span class="line"></span><br><span class="line">        label = batch[<span class="string">&#x27;label&#x27;</span>].to(device) <span class="comment"># (batch, seq_len)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># (batch, seq_len, tgt_vocab_size) -[view]-&gt; (batch * seq_len, tgt_vocab_size)</span></span><br><span class="line">        <span class="comment"># (batch, seq_len) -[view]-&gt; (batch * seq_len)</span></span><br><span class="line">        loss = loss_fn(proj_output.view(-<span class="number">1</span>, tokenizer_tgt.get_vocab_size()), label.view(-<span class="number">1</span>))</span><br><span class="line">        batch_iterator.set_postfix(&#123;<span class="string">f&quot;loss&quot;</span>: <span class="string">f&quot;<span class="subst">&#123;loss.item():<span class="number">6.3</span>f&#125;</span>&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 为TensorBoard记录loss</span></span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;train loss&#x27;</span>, loss.item(), global_step)</span><br><span class="line">        writer.flush()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># loss反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        global_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在每个epoch结束后保存模型</span></span><br><span class="line">    <span class="comment"># 保存模型是个好习惯，否则每次训练都需要从零开始</span></span><br><span class="line">    model_filename = get_weights_file_path(config, <span class="string">f&quot;<span class="subst">&#123;epoch:02d&#125;</span>&quot;</span>)</span><br><span class="line">    torch.save(&#123;</span><br><span class="line">        <span class="string">&#x27;epoch&#x27;</span>: epoch,</span><br><span class="line">        <span class="string">&#x27;model_state_dict&#x27;</span>: model.state_dict(),</span><br><span class="line">        <span class="string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),</span><br><span class="line">        <span class="string">&#x27;global_step&#x27;</span>: global_step,</span><br><span class="line">    &#125;, model_filename)</span><br></pre></td></tr></table></figure>

<p>编写<code>__main__</code>函数执行训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    config = get_config()</span><br><span class="line">    train_model(config)</span><br></pre></td></tr></table></figure>

<p>此时就可以正常的开始模型训练了，如果编写正确的话，程序就会自动下载数据集并开始训练了。</p>
<p>在训练之前，我们还可以可视化训练的效果，比如查看模型预测的结果是否与真实结果是否一致，也就是validation，即观察模型在训练时是如何演化的。</p>
<h2 id="4-Validation-Loop"><a href="#4-Validation-Loop" class="headerlink" title="4 Validation Loop"></a>4 Validation Loop</h2><p>接下来编写validation loop，以便我们可以实时评估模型推理结果，观察模型是如何翻译数据集中并不在训练集当中的句子的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_validation</span>(<span class="params">model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=<span class="number">2</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 告诉pytorch将模型置于eval模式，关闭dropout、batchnorm中的随机因素</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># 测试2个句子看看模型输出的翻译结果是什么</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    srouce_texts = []</span><br><span class="line">    expected = []</span><br><span class="line">    predicted = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 终端宽度</span></span><br><span class="line">    console_width = <span class="number">80</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 关闭pytorch的梯度计算，在这个`with`中我们只需要推理结果，不需要训练，因此不需要计算tensor的梯度</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> validation_ds:</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">            encoder_input = batch[<span class="string">&#x27;encoder_input&#x27;</span>].to(device) <span class="comment"># (batch, seq_len)</span></span><br><span class="line">            encoder_mask = batch[<span class="string">&#x27;encoder_mask&#x27;</span>].to(device) <span class="comment"># (batch, 1, 1, seq_len) 只隐藏padding tokens</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 确定validation的batch应为1</span></span><br><span class="line">            <span class="keyword">assert</span> encoder_input.size(<span class="number">0</span>) == <span class="number">1</span>, <span class="string">&quot;Batch size must be 1 for validation&quot;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 当模型进行推理时，只需要计算一次`encoder_output`，然后重复使用它来为每个token计算`decoder_output`</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>回忆Transformer模型，当我们想用模型进行推理时，只需要计算一次<code>encoder_output</code>，然后重复使用它来为每个token计算<code>decoder_output</code>，现在编写<code>greedy_decode</code>函数，用于生成翻译结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_decode</span>(<span class="params">model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用贪婪策略，缓存encoder输出，计算decoder输出</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数: </span></span><br><span class="line"><span class="string">        `model`: 模型</span></span><br><span class="line"><span class="string">        `source`: 源语言句子</span></span><br><span class="line"><span class="string">        `source_mask`: 源语言句子的padding mask</span></span><br><span class="line"><span class="string">        `tokenizer_src`: 源语言的分词器</span></span><br><span class="line"><span class="string">        `tokenizer_tgt`: 目标语言的分词器</span></span><br><span class="line"><span class="string">        `max_len`: 句子最大长度</span></span><br><span class="line"><span class="string">        `device`: 运行模型的设备</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 告诉pytorch将模型置于eval模式，关闭dropout、batchnorm中的随机因素</span></span><br><span class="line">    sos_idx = tokenizer_tgt.token_to_id(<span class="string">&#x27;[SOS]&#x27;</span>)</span><br><span class="line">    eos_idx = tokenizer_tgt.token_to_id(<span class="string">&#x27;[EOS]&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算encoder输出，并将结果重用于计算每个token的decoder输出</span></span><br><span class="line">    encoder_output = model.encode(source, source_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 模型推理计算过程如下：首先输入句子的起始符[SOS]作为decoder的第一个输入，然后得到模型输出的翻译句子的第一个词；</span></span><br><span class="line">    <span class="comment"># 接下在每一步迭代中将上一步的输出附加在输入中，使模型继续输出再下一个词，直到遇到[EOS]或达到句子的最大长度。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用[SOS]初始化decoder输入</span></span><br><span class="line">    decoder_input = torch.empty(<span class="number">1</span>, <span class="number">1</span>).fill_(sos_idx).type_as(source).to(device)</span><br><span class="line">    <span class="comment"># 接下来让decoder循环输出下一个词，知道遇到[EOS]或达到句子最大长度</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判断是否达到句子最大长度</span></span><br><span class="line">        <span class="keyword">if</span> decoder_input.size(<span class="number">1</span>) == max_len:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 为target（即decoder input）创建mask，使模型只能看到当前时刻之前的词</span></span><br><span class="line">        <span class="comment"># 对比`dataset.py`中使用的`causal_mask`，这里使用的是`decoder_input.size(1)`作为`tgt_mask`的长度，并不需要使用额外的padding</span></span><br><span class="line">        decoder_mask = causal_mask(decoder_input.size(<span class="number">1</span>)).type_as(source_mask).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算decoder输出，这里复用了`encoder_output`</span></span><br><span class="line">        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask) <span class="comment"># (1, seq_len, d_model)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算下一个词：使用projection层计算下一个token的概率分布，只取最后一个词的projection，out[:, -1]为(1, 1, d_model)，prob为(1, 1, vocab_size)</span></span><br><span class="line">        prob = model.project(out[:, -<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 取概率最大的词作为下一个token（这里使用贪婪策略，即只取概率最大的词，而不是所有词的概率之和）</span></span><br><span class="line">        _, next_word = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 再将next_word添加到decoder_input的末尾</span></span><br><span class="line">        decoder_input = torch.cat([decoder_input, torch.empty(<span class="number">1</span>, <span class="number">1</span>).type_as(source).fill_(next_word.item()).to(device)], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果遇到[EOS]，则停止循环</span></span><br><span class="line">        <span class="keyword">if</span> next_word == eos_idx:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> decoder_input.squeeze(<span class="number">0</span>) <span class="comment"># 输出翻译句子</span></span><br></pre></td></tr></table></figure>

<p>继续编写<code>run_validation</code>函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确定validation的batch应为1</span></span><br><span class="line"><span class="keyword">assert</span> encoder_input.size(<span class="number">0</span>) == <span class="number">1</span>, <span class="string">&quot;Batch size must be 1 for validation&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当我们想要模型进行推理时，我们只需要计算一次`encoder_output`，然后重复使用它来为每个token计算`decoder_output`</span></span><br><span class="line">model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)</span><br><span class="line"></span><br><span class="line">source_text = batch[<span class="string">&#x27;src_text&#x27;</span>][<span class="number">0</span>]</span><br><span class="line">target_text = batch[<span class="string">&#x27;tgt_text&#x27;</span>][<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 用目标语言的分词器将模型输出的token序列转换为文本</span></span><br><span class="line">model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())</span><br><span class="line"></span><br><span class="line">source_texts.append(source_text)</span><br><span class="line">expected.append(target_text)</span><br><span class="line">predicted.append(model_out_text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印输出，为了不影响`tqdm`进度条正常刷新，我们使用`print_msg`而不是`print`来打印输出。</span></span><br><span class="line">print_msg(<span class="string">&#x27;-&#x27;</span>*console_width)</span><br><span class="line">print_msg(<span class="string">f&quot;Source text: <span class="subst">&#123;source_text&#125;</span>&quot;</span>)</span><br><span class="line">print_msg(<span class="string">f&quot;TARGET text: <span class="subst">&#123;target_text&#125;</span>&quot;</span>)</span><br><span class="line">print_msg(<span class="string">f&quot;PREDICTED text: <span class="subst">&#123;model_out_text&#125;</span>&quot;</span>)</span><br><span class="line">print_msg(<span class="string">&#x27;-&#x27;</span>*console_width)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果预测了足够的句子，则停止循环</span></span><br><span class="line"><span class="keyword">if</span> count &gt;= num_examples:</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>最后，我们可以将<code>run_validation</code>函数添加到<code>train_model</code>函数中，放在每个epoch结束后：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">    global_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config[<span class="string">&#x27;seq_len&#x27;</span>], device, <span class="keyword">lambda</span> msg: batch_iterator.write(msg), global_step, writer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在每个epoch结束后保存模型</span></span><br><span class="line"><span class="comment"># 保存模型是个好习惯，否则每次训练都需要从零开始</span></span><br><span class="line">model_filename = get_weights_file_path(config, <span class="string">f&quot;<span class="subst">&#123;epoch:02d&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="5-Attention-Visualization"><a href="#5-Attention-Visualization" class="headerlink" title="5 Attention Visualization"></a>5 Attention Visualization</h2><p>可视化的代码中的大部分功能都不需要我们自己实现，前人已经帮我们完成了很多工作。这里使用jupyter notebook，新建<code>attention_visual.ipynb</code>文件。</p>
<p>这里除了之前我们自己写的模型和训练库以外，还使用了<code>altair</code>可视化库。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> Transformer</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> get_config, get_weights_file_path</span><br><span class="line"><span class="keyword">from</span> train <span class="keyword">import</span> get_model, get_ds, greedy_decode</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> altair <span class="keyword">as</span> alt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>同之前一样，选择计算设备：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从`train_model`中拷贝选择设备的代码即可</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Using device: <span class="subst">&#123;device&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>加载预训练模型的权重：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">config = get_config()</span><br><span class="line">train_dataloader, val_dataloader, vocab_src, vocab_tgt = get_ds(config)</span><br><span class="line">model = get_model(config, vocab_src.get_vocab_size(), vocab_tgt.get_vocab_size()).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练权重</span></span><br><span class="line">model_filename = get_weights_file_path(config, <span class="string">f&quot;03&quot;</span>)</span><br><span class="line">state = torch.load(model_filename)</span><br><span class="line">model.load_state_dict(state[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p>从验证集中选择一对样本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_next_batch</span>():</span></span><br><span class="line">    <span class="comment"># Load a sample batch from the validation set</span></span><br><span class="line">    batch = <span class="built_in">next</span>(<span class="built_in">iter</span>(val_dataloader))</span><br><span class="line">    encoder_input = batch[<span class="string">&quot;encoder_input&quot;</span>].to(device)</span><br><span class="line">    encoder_mask = batch[<span class="string">&quot;encoder_mask&quot;</span>].to(device)</span><br><span class="line">    decoder_input = batch[<span class="string">&quot;decoder_input&quot;</span>].to(device)</span><br><span class="line">    decoder_mask = batch[<span class="string">&quot;decoder_mask&quot;</span>].to(device)</span><br><span class="line"></span><br><span class="line">    encoder_input_tokens = [vocab_src.id_to_token(idx) <span class="keyword">for</span> idx <span class="keyword">in</span> encoder_input[<span class="number">0</span>].cpu().numpy()]</span><br><span class="line">    decoder_input_tokens = [vocab_tgt.id_to_token(idx) <span class="keyword">for</span> idx <span class="keyword">in</span> decoder_input[<span class="number">0</span>].cpu().numpy()]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># check that the batch size is 1</span></span><br><span class="line">    <span class="keyword">assert</span> encoder_input.size(</span><br><span class="line">        <span class="number">0</span>) == <span class="number">1</span>, <span class="string">&quot;Batch size must be 1 for validation&quot;</span></span><br><span class="line"></span><br><span class="line">    model_out = greedy_decode(</span><br><span class="line">        model, encoder_input, encoder_mask, vocab_src, vocab_tgt, config[<span class="string">&#x27;seq_len&#x27;</span>], device)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> batch, encoder_input_tokens, decoder_input_tokens</span><br></pre></td></tr></table></figure>

<p>可视化注意力矩阵的函数，这些函数基本都能从网上找到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mtx2df</span>(<span class="params">m, max_row, max_col, row_tokens, col_tokens</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    生成一个可视化矩阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(</span><br><span class="line">        [</span><br><span class="line">            (</span><br><span class="line">                r,</span><br><span class="line">                c,</span><br><span class="line">                <span class="built_in">float</span>(m[r, c]),</span><br><span class="line">                <span class="string">&quot;%.3d %s&quot;</span> % (r, row_tokens[r] <span class="keyword">if</span> <span class="built_in">len</span>(row_tokens) &gt; r <span class="keyword">else</span> <span class="string">&quot;&lt;blank&gt;&quot;</span>),</span><br><span class="line">                <span class="string">&quot;%.3d %s&quot;</span> % (c, col_tokens[c] <span class="keyword">if</span> <span class="built_in">len</span>(col_tokens) &gt; c <span class="keyword">else</span> <span class="string">&quot;&lt;blank&gt;&quot;</span>),</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(m.shape[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(m.shape[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> r &lt; max_row <span class="keyword">and</span> c &lt; max_col</span><br><span class="line">        ],</span><br><span class="line">        columns=[<span class="string">&quot;row&quot;</span>, <span class="string">&quot;column&quot;</span>, <span class="string">&quot;value&quot;</span>, <span class="string">&quot;row_token&quot;</span>, <span class="string">&quot;col_token&quot;</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_attn_map</span>(<span class="params">attn_type: <span class="built_in">str</span>, layer: <span class="built_in">int</span>, head: <span class="built_in">int</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    从指定的层和头获取attention map</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 我们在`model.py`的`MultiHeadAttentionBlock`中，定义`attention`函数的返回值时，同时返回了`attention_scores`，</span></span><br><span class="line">    <span class="comment"># 之后又在`forward`中将其保存在`self.attention_scores`中。</span></span><br><span class="line">    <span class="comment"># 因此，可以通过`model.encoder.layers[layer].self_attention_block.attention_scores`</span></span><br><span class="line">    <span class="comment"># 来获取encoder的self-attention的attention_scores。</span></span><br><span class="line">    <span class="keyword">if</span> attn_type == <span class="string">&quot;encoder&quot;</span>:</span><br><span class="line">        attn = model.encoder.layers[layer].self_attention_block.attention_scores</span><br><span class="line">    <span class="keyword">elif</span> attn_type == <span class="string">&quot;decoder&quot;</span>:</span><br><span class="line">        attn = model.decoder.layers[layer].self_attention_block.attention_scores</span><br><span class="line">    <span class="keyword">elif</span> attn_type == <span class="string">&quot;encoder-decoder&quot;</span>:</span><br><span class="line">        attn = model.decoder.layers[layer].cross_attention_block.attention_scores</span><br><span class="line">    <span class="keyword">return</span> attn[<span class="number">0</span>, head].data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attn_map</span>(<span class="params">attn_type, layer, head, row_tokens, col_tokens, max_sentence_len</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用altair绘制attention map</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    df = mtx2df(</span><br><span class="line">        get_attn_map(attn_type, layer, head),</span><br><span class="line">        max_sentence_len,</span><br><span class="line">        max_sentence_len,</span><br><span class="line">        row_tokens,</span><br><span class="line">        col_tokens,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(data=df)</span><br><span class="line">        .mark_rect()</span><br><span class="line">        .encode(</span><br><span class="line">            x=alt.X(<span class="string">&quot;col_token&quot;</span>, axis=alt.Axis(title=<span class="string">&quot;&quot;</span>)),</span><br><span class="line">            y=alt.Y(<span class="string">&quot;row_token&quot;</span>, axis=alt.Axis(title=<span class="string">&quot;&quot;</span>)),</span><br><span class="line">            color=<span class="string">&quot;value&quot;</span>,</span><br><span class="line">            tooltip=[<span class="string">&quot;row&quot;</span>, <span class="string">&quot;column&quot;</span>, <span class="string">&quot;value&quot;</span>, <span class="string">&quot;row_token&quot;</span>, <span class="string">&quot;col_token&quot;</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">#.title(f&quot;Layer &#123;layer&#125; Head &#123;head&#125;&quot;)</span></span><br><span class="line">        .properties(height=<span class="number">400</span>, width=<span class="number">400</span>, title=<span class="string">f&quot;Layer <span class="subst">&#123;layer&#125;</span> Head <span class="subst">&#123;head&#125;</span>&quot;</span>)</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_attention_maps</span>(<span class="params">attn_type: <span class="built_in">str</span>, layers: <span class="built_in">list</span>[<span class="built_in">int</span>], heads: <span class="built_in">list</span>[<span class="built_in">int</span>], row_tokens: <span class="built_in">list</span>, col_tokens, max_sentence_len: <span class="built_in">int</span></span>):</span></span><br><span class="line">    charts = []</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">        rowCharts = []</span><br><span class="line">        <span class="keyword">for</span> head <span class="keyword">in</span> heads:</span><br><span class="line">            rowCharts.append(attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len))</span><br><span class="line">        charts.append(alt.hconcat(*rowCharts))</span><br><span class="line">    <span class="keyword">return</span> alt.vconcat(*charts)</span><br></pre></td></tr></table></figure>

<p>获取一对样本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch, encoder_input_tokens, decoder_input_tokens = load_next_batch()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Source: <span class="subst">&#123;batch[<span class="string">&quot;src_text&quot;</span>][<span class="number">0</span>]&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Target: <span class="subst">&#123;batch[<span class="string">&quot;tgt_text&quot;</span>][<span class="number">0</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">sentence_len = encoder_input_tokens.index(<span class="string">&quot;[PAD]&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>绘制Encoder的self-attention：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">layers = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">heads = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化Encoder的Self-Attention</span></span><br><span class="line">get_all_attention_maps(<span class="string">&quot;encoder&quot;</span>, layers, heads, encoder_input_tokens, encoder_input_tokens, <span class="built_in">min</span>(<span class="number">20</span>, sentence_len))</span><br></pre></td></tr></table></figure>

<p>绘制Decoder的self-attention：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化Decoder的Self-Attention</span></span><br><span class="line">get_all_attention_maps(<span class="string">&quot;decoder&quot;</span>, layers, heads, decoder_input_tokens, decoder_input_tokens, <span class="built_in">min</span>(<span class="number">20</span>, sentence_len))</span><br></pre></td></tr></table></figure>

<p>绘制Encoder-Decoder的cross-attention：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化Decoder的Cross-Attention</span></span><br><span class="line"><span class="comment"># 这是实际上计算en-it翻译任务的地方，cross attention使用encoder的key/value，以及decoder的query计算attention。</span></span><br><span class="line">get_all_attention_maps(<span class="string">&quot;encoder-decoder&quot;</span>, layers, heads, encoder_input_tokens, decoder_input_tokens, <span class="built_in">min</span>(<span class="number">20</span>, sentence_len))</span><br></pre></td></tr></table></figure></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Python/">Python</a><a class="link-muted mr-2" rel="tag" href="/tags/Linux/">Linux</a><a class="link-muted mr-2" rel="tag" href="/tags/llama/">llama</a></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/alipay.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wechatpay.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2024/01/06/transformer-from-scratch-1/"><span class="level-item">理解Transformer模型1：编写Transformer</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><script src="https://utteranc.es/client.js" repo="zlotus/comment" issue-term="pathname" label="some-issue-label" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/opm_sq.png" alt="子实"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">子实</p><p class="is-size-6 is-block">程序员</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>陕西/西安</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">81</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">23</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/zlotus" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/zlotus"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/qinzishi"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/Zealot_uS"><i class="fab fa-twitter"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://www.zhihu.com/people/zlotus" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">知乎</span></span><span class="level-right"><span class="level-item tag">www.zhihu.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Diary/"><span class="level-start"><span class="level-item">Diary</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">67</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Coroutines/"><span class="tag">Coroutines</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JavaScript/"><span class="tag">JavaScript</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Javascript/"><span class="tag">Javascript</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">33</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenCV/"><span class="tag">OpenCV</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Operating-System/"><span class="tag">Operating System</span><span class="tag">21</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Paper/"><span class="tag">Paper</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Raspberry-Pi/"><span class="tag">Raspberry Pi</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Rust/"><span class="tag">Rust</span><span class="tag">29</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Shell/"><span class="tag">Shell</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TensorFlow/"><span class="tag">TensorFlow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tokio/"><span class="tag">Tokio</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ZMQ/"><span class="tag">ZMQ</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ctpn/"><span class="tag">ctpn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/llama/"><span class="tag">llama</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/macOS/"><span class="tag">macOS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/obsidion/"><span class="tag">obsidion</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%97%A5%E8%AE%B0/"><span class="tag">日记</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9D%82%E8%B0%88/"><span class="tag">杂谈</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BC%96%E7%A8%8B/"><span class="tag">编程</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%84%9A%E6%9C%AC/"><span class="tag">脚本</span><span class="tag">4</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/opm_sq.png" alt="子实" height="28"></a><p class="is-size-7"><span>&copy; 2024 子实</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>